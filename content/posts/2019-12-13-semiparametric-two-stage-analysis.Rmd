---
title: Semiparametric Two-Stage Analysis - Introduction
author: ''
date: '2019-12-13'
slug: semiparametric-two-stage-1
categories: []
tags: []
description: ''
externalLink: ''
series: []
bibliography: [semi.bib]
link-citations: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(RColorBrewer)
```

This past fall I had the opportunity to take [BIOSTAT880](https://sph.umich.edu/academics/courses/course.php?courseID=BIOSTAT880): Statistical Analysis with Missing Data. Although the course did cover a broad overview of different approaches to missing data analysis, we spent a decent amount of time covering various semiparametric approaches, from inverse probability weighting (IPW) and augmented IPW approaches to [calibration-based approaches](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1002.7547&rep=rep1&type=pdf). This wasn't my first taste of semiparametric inference, but it wasn't a topic I felt entirely comfortable with, so for the term project I did for the course I choose to do a review of semiparametric approaches to the analysis of data from two-stage designs with a focus on applications to two-stage designs in genetics. I found the papers I read on the topic were incredibly interesting, so I figured that along with the term paper I'd also write a series blog post that covers the material in a more informal matter. First I'll cover two-stage designs, with a particular emphasis on two-stage outcome-dependent designs. 

# Two-stage designs

In a two-stage design, there is a first phase in which a sample is drawn from a population of interest, and then a smaller subsample is then sampled from this sample for further analysis in the second phase of a study. This is generally done when a measure of interest is expensive to acquire and there are cheaper/easier to acquire measures that are correlated with the expensive variable. @neyman1938 considered the situation where the outcome of interest was expensive to acquire (due to a long interview), but there were other variables that were both easier to acquire for a larger sample and were correlated with the outcome of interest. Here one strategy would be to collect information on the cheap covariates for a large sample and then do stratified sampling on this sample to chose who to measure the outcome on. He explored examples where this double sampling approach could be either helpful or not in terms of efficiency (as opposed to doing one simple random sample to measure the outcome), which depends on prior knowledge on the relationship between the covariates in the population.

# Two-stage outcome dependent designs

Sometimes the outcome of interest is easy to acquire, but it is postulated that there is a relationship between the outcome and a set of covariates that is expensive to acquire. To tie all of this to genetics, we can easily think of settings where we might have trait or disease measures on a large population that we think might have a genetic component, but it infeasible to genotype every subject. @white1982 introduced the idea of a two-stage outcome-dependent design through the example of a rare-exposure/rare-disease study. In such a scenario, we might have a rare disease and rare exposure, and we might want to assess risk while controlling for other (discrete) covariates. To get reasonable estimates of risk in this scenario, a simple random sample from a population would have to be rather large in order to sample both enough cases and exposed individuals in each strata determined by the other covariates. This would lead to highly variable estimates of stratum-specific odds ratios and could potentially reduce power to detect a covariate-adjusted association between the disease and the risk factor. Here we're interested in how the subjects are distributed into the strata based on their disease and exposure status, and collecting enough information on exposed subjects to do this in a simple random sample would entail sampling way more unexposed individuals than we'd need to do the same for them. @white1982 proposed a two-stage approach in which during the first stage data are collected on exposure and disease status from a large sample. This sample can be represented in a 2x2 table:

<center>

|  | |Disease| |
|:-:|:-:|:-:|:-:|
|Exposure  | | Y | N |
|  | Y | $N_1$ | $N_2$ |
|  | N | $N_3$ | $N_4$ |

</center>

In the second stage of the study separate random samples are taken from each of the cells, and these subjects have data on additional covariates measured. The suggestion made in @white1982 is to over-sample from the smaller cells, the idea being that rarer observations are more informative. In the paper an example is given where the four cell counts are 50, 30, 250, and 270; the suggestion for a second stage sample of 400 subjects from this first sample is to take 50, 30, 160 and 160 individuals from the cells, respectively. 

@white1982 lists two conditions for this two-stage approach to be advantageous over a simple random sample:

1. Small counts are expected for some groups/strata determined by the covariates of interest
2. The two-stage design would save money, meaning that collection of the disease/exposure status is cheaper than collecting the disease/exposure status and the other covariates in one pass.

In generalizing this design to settings where the other covariates aren't necessarily discrete, I'm under the impression that point 2 is what's really put into consideration (I'm not entirely sure how point 1 would generalize outside of the above setting). Also, interest might be in evaluating the relationship between expensive covariates and continuous outcomes. In this case a suggestion for the sampling mechanism for the second stage is to employ an "extreme tails" approach where subjects with lower or higher than expected values of the trait of interest are selected for further measure. For example, in the following toy example where we're interested in relating height to a set of expensive covariates, we might start with a sample with the following distribution:

```{r fig1, echo=FALSE, fig.width=10}
set.seed(101)
lil_people <- rnorm(1000, 48, 2) - 4
dens <- density(lil_people)
dd <- with(dens,data.frame(x,y))
q75 <- quantile(lil_people, .75)
q95 <- quantile(lil_people, .95)

qplot(x,y,data=dd,geom="line") +
  theme_light() +
  xlab("Height (in)") + 
  ylab("Density") +
  ggtitle("Distribution of Height") +
  theme(plot.title = element_text(hjust = 0.5, size=20),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))
```

If we were to employ an extreme-tails approach, we'd select subjects falling into either tail of the distribution, for example:

```{r fig2, echo=FALSE, fig.width=10}
set.seed(101)
lil_people <- rnorm(1000, 48, 2) - 4
dens <- density(lil_people)
dd <- with(dens,data.frame(x,y))
q75 <- quantile(lil_people, .75)
q95 <- quantile(lil_people, .95)

qplot(x,y,data=dd,geom="line") +
  geom_ribbon(data=subset(dd,x < 42), aes(ymax=y), ymin=0,
              fill="#20B2AA",colour=NA,alpha=1) +
  geom_ribbon(data=subset(dd,x > 46), aes(ymax=y), ymin=0,
              fill="#20B2AA",colour=NA,alpha=1) +
  theme_light() +
  xlab("Height (in)") + 
  ylab("Density") +
  ggtitle("Distribution of Height") +
  theme(plot.title = element_text(hjust = 0.5, size=20),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))
```

Note that we could do something similar but also account for other covariates and allow for an individually determined threshold; see @bjornland2018, for example.

Now that we've discussed what a two-stage outcome-dependent design might look like, we next look at how to analyze data while also accounting for the selection mechanism. Semiparametric approaches to this will be the subject of the next post in this series.

# References